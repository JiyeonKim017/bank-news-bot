import os
import requests
import yfinance as yf
from datetime import datetime
from collections import Counter
import re

# 1. í™˜ê²½ ë³€ìˆ˜
CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# 2. ì§€ì •ëœ ê²½ì œì§€ ë„ë©”ì¸ (ì´ë¯¸ì§€ ê¸°ë°˜ 12ê³³ + ì£¼ìš” í†µì‹ ì‚¬)
MEDIA_MAP = {
    "mk.co.kr": "ë§¤ì¼ê²½ì œ",
    "hankyung.com": "í•œêµ­ê²½ì œ",
    "sedaily.com": "ì„œìš¸ê²½ì œ",
    "mt.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´",
    "edaily.co.kr": "ì´ë°ì¼ë¦¬",
    "fnnews.com": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
    "bizwatch.co.kr": "ë¹„ì¦ˆì›Œì¹˜",
    "chosunbiz.com": "ì¡°ì„ ë¹„ì¦ˆ",
    "asiae.co.kr": "ì•„ì‹œì•„ê²½ì œ",
    "heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ",
    "dnews.co.kr": "ëŒ€í•œê²½ì œ",
    "joseilbo.com": "ì¡°ì„¸ì¼ë³´",
    "yna.co.kr": "ì—°í•©ë‰´ìŠ¤",
    "news1.kr": "ë‰´ìŠ¤1",
    "newsis.com": "ë‰´ì‹œìŠ¤"
}

def get_financial_indicators():
    try:
        usd_krw = yf.Ticker("USDKRW=X")
        curr = usd_krw.history(period="1d")['Close'].iloc[-1]
        kospi = yf.Ticker("^KS11")
        k_val = kospi.history(period="1d")['Close'].iloc[-1]
        return f"{curr:,.2f}", f"{k_val:,.2f}"
    except:
        return "ë°ì´í„° í™•ì¸ ì¤‘", "ë°ì´í„° í™•ì¸ ì¤‘"

def get_news(query):
    # ìµœì‹ ìˆœ ì •ë ¬ë¡œ 100ê°œë¥¼ ê°€ì ¸ì™€ì„œ í•„í„°ë§ ì¤€ë¹„
    url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=100&sort=date"
    headers = {"X-Naver-Client-Id": CLIENT_ID, "X-Naver-Client-Secret": CLIENT_SECRET}
    res = requests.get(url, headers=headers)
    return res.json().get('items', []) if res.status_code == 200 else []

def main():
    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    queries = ["ì‹œì¤‘ì€í–‰ ê¸ˆë¦¬", "ì€í–‰ DX", "ê°€ê³„ëŒ€ì¶œ ê·œì œ"]
    
    rate, kospi = get_financial_indicators()
    all_titles, news_section = [], ""

    for q in queries:
        items = get_news(q)
        news_section += f"#### ğŸ” '{q}' ì„¹ì…˜\n| ë‚ ì§œ | ì–¸ë¡ ì‚¬ | ë‰´ìŠ¤ ì œëª© |\n| :--- | :--- | :--- |\n"
        
        unique_titles = set()
        count = 0
        for item in items:
            link = item['link']
            media_name = None
            for domain, name in MEDIA_MAP.items():
                if domain in link:
                    media_name = name
                    break
            
            if not media_name: continue

            title = re.sub(r'<[^>]*>', '', item['title']).replace('&quot;', '"').replace('&apos;', "'")
            if title not in unique_titles and count < 5:
                date = item['pubDate'][5:16]
                news_section += f"| {date} | {media_name} | [{title}]({link}) |\n"
                all_titles.append(title)
                unique_titles.add(title)
                count += 1
        
        if count == 0:
            news_section += "| - | - | ìµœê·¼ 100ê°œ ê¸°ì‚¬ ì¤‘ ì§€ì • ê²½ì œì§€ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. |\n"
        news_section += "\n"

    # íŠ¸ë Œë“œ ë¶„ì„
    words = []
    for t in all_titles:
        clean = re.sub(r'[^ê°€-í£a-zA-Z\s]', '', t)
        words.extend([w for w in clean.split() if len(w) >= 2])
    trends = [f"`#{tag}`" for tag, _ in Counter(words).most_common(5)]

    readme = f"""# ğŸ¦ ê¸ˆìœµ ë‰´ìŠ¤ íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ

> **ì—…ë°ì´íŠ¸:** `{now}` (KST)  
> **ìë™ ìŠ¤ì¼€ì¤„:** ë§¤ì¼ 09:00, 14:00, 17:00 (KST)

---

### ğŸ”¥ ì˜¤ëŠ˜ì˜ í•µì‹¬ í‚¤ì›Œë“œ (ë¶„ì„)
{" ".join(trends)}

---

### ğŸ“ˆ ì£¼ìš” ê²½ì œ ì§€í‘œ
| ì§€í‘œëª… | í˜„ì¬ê°€ |
| :--- | :---: |
| **USD/KRW í™˜ìœ¨** | {rate}ì› |
| **ì½”ìŠ¤í”¼ ì§€ìˆ˜** | {kospi} |

---

### ğŸ“° ì‹¤ì‹œê°„ ë‰´ìŠ¤ (ê²½ì œì§€ í•„í„°ë§)
{news_section}

---
*ì œì‘: JiyeonKim017 / GitHub Actions ìë™í™”*
"""
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(readme)

if __name__ == "__main__":
    main()
