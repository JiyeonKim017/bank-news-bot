import os
import requests
import yfinance as yf
from datetime import datetime
from collections import Counter
import re
import random

# 1. í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# ì§€ì—°ë‹˜ì´ ì§€ì •í•œ 11ê°œ ê²½ì œì§€ + 3ê°œ í†µì‹ ì‚¬
MEDIA_MAP = {
    "mk.co.kr": "ë§¤ì¼ê²½ì œ", "mt.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´", "bizwatch.co.kr": "ë¹„ì¦ˆì›Œì¹˜",
    "sedaily.com": "ì„œìš¸ê²½ì œ", "asiae.co.kr": "ì•„ì‹œì•„ê²½ì œ", "edaily.co.kr": "ì´ë°ì¼ë¦¬",
    "chosunbiz.com": "ì¡°ì„ ë¹„ì¦ˆ", "joseilbo.com": "ì¡°ì„¸ì¼ë³´", "fnnews.com": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
    "hankyung.com": "í•œêµ­ê²½ì œ", "heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ",
    "yna.co.kr": "ì—°í•©ë‰´ìŠ¤", "news1.kr": "ë‰´ìŠ¤1", "newsis.com": "ë‰´ì‹œìŠ¤"
}

def get_financial_indicators():
    try:
        usd_krw = yf.Ticker("USDKRW=X")
        rate = usd_krw.history(period="1d")['Close'].iloc[-1]
        kospi = yf.Ticker("^KS11")
        k_val = kospi.history(period="1d")['Close'].iloc[-1]
        return f"{rate:,.2f}", f"{k_val:,.2f}"
    except:
        return "ë°ì´í„° í™•ì¸ ì¤‘", "ë°ì´í„° í™•ì¸ ì¤‘"

def get_news_by_press():
    all_items = []
    headers = {"X-Naver-Client-Id": CLIENT_ID, "X-Naver-Client-Secret": CLIENT_SECRET}
    
    # [í•µì‹¬] ì–¸ë¡ ì‚¬ ì´ë¦„ì„ ì§ì ‘ ê²€ìƒ‰ì–´ë¡œ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ ë§¤ì²´ì˜ ìµœì‹  ë‰´ìŠ¤ë¥¼ ìœ ë„í•©ë‹ˆë‹¤.
    search_queries = ["ë§¤ì¼ê²½ì œ", "í•œêµ­ê²½ì œ", "ë¨¸ë‹ˆíˆ¬ë°ì´", "ì—°í•©ë‰´ìŠ¤", "ê¸ˆìœµ", "ì¦ê¶Œ"]
    
    for q in search_queries:
        url = f"https://openapi.naver.com/v1/search/news.json?query={q}&display=100&sort=date"
        res = requests.get(url, headers=headers)
        if res.status_code == 200:
            all_items.extend(res.json().get('items', []))
    return all_items

def main():
    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    rate, kospi = get_financial_indicators()
    
    # 1. ê¸°ì‚¬ ìˆ˜ì§‘ ë° í•„í„°ë§
    raw_items = get_news_by_press()
    filtered_news = []
    unique_titles = set()

    for item in raw_items:
        full_link = item.get('originallink', '') + item.get('link', '')
        media_name = next((name for domain, name in MEDIA_MAP.items() if domain in full_link), None)
        
        if media_name:
            title = re.sub(r'<[^>]*>', '', item['title']).replace('&quot;', '"').replace('&apos;', "'")
            if title not in unique_titles:
                filtered_news.append({
                    "date": item['pubDate'][5:16],
                    "media": media_name,
                    "title": title,
                    "link": item['link']
                })
                unique_titles.add(title)

    # 2. í‚¤ì›Œë“œ ë¶„ì„ (ì „ì²´ ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì œëª© ê¸°ì¤€)
    words = []
    stopwords = ['ë‰´ìŠ¤', 'ì˜¤ëŠ˜', 'ê¸°ì', 'ì˜¤ì „', 'ì˜¤í›„', 'ë¶„ì„', 'ê²½ì œ', 'ê¸ˆìœµ']
    for n in filtered_news:
        clean = re.sub(r'[^ê°€-í£\s]', '', n['title'])
        words.extend([w for w in clean.split() if len(w) >= 2 and w not in stopwords])
    
    # ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ìƒìœ„ 6ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ
    trends = [f"`#{tag}`" for tag, _ in Counter(words).most_common(6)]

    # 3. ë‰´ìŠ¤ í…Œì´ë¸” êµ¬ì„± (ìµœì‹  12ê°œë§Œ)
    news_table = "| ë‚ ì§œ | ì–¸ë¡ ì‚¬ | ë‰´ìŠ¤ í—¤ë“œë¼ì¸ |\n| :--- | :--- | :--- |\n"
    for n in filtered_news[:12]:
        news_table += f"| {n['date']} | {n['media']} | [{n['title']}]({n['link']}) |\n"

    # 4. README ì‘ì„±
    readme = f"""# ğŸ¦ ì‹¤ì‹œê°„ ê¸ˆìœµ ë‰´ìŠ¤ ëŒ€ì‹œë³´ë“œ

> **ì—…ë°ì´íŠ¸:** `{now}` (KST)

---

### ğŸ”¥ ë‰´ìŠ¤ ê¸°ë°˜ í•« í‚¤ì›Œë“œ
{" ".join(trends)}  
*ì–¸ë¡ ì‚¬ë³„ ìµœì‹  ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ì—¬ ì¶”ì¶œí•œ í‚¤ì›Œë“œì…ë‹ˆë‹¤.*

---

### ğŸ“ˆ ì£¼ìš” ì§€í‘œ
| ì§€í‘œëª… | í˜„ì¬ê°€ |
| :--- | :---: |
| **USD/KRW í™˜ìœ¨** | {rate}ì› |
| **ì½”ìŠ¤í”¼ ì§€ìˆ˜** | {kospi} |

---

### ğŸ“° ì‹¤ì‹œê°„ ì£¼ìš” ë‰´ìŠ¤ (14ê°œ ë§¤ì²´ íƒ€ê²ŸíŒ…)
{news_table}

---
*ì œì‘: JiyeonKim017 / 11ê°œ ê²½ì œì§€ ë° 3ê°œ í†µì‹ ì‚¬ ë°ì´í„° ê¸°ë°˜*
"""
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(readme)

if __name__ == "__main__":
    main()
