import os
import requests
import yfinance as yf
from datetime import datetime
from collections import Counter
import re

# 1. í™˜ê²½ ë³€ìˆ˜
CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# 2. ì§€ì •ëœ ê²½ì œì§€ ë° ì£¼ìš” ì–¸ë¡ ì‚¬ ë„ë©”ì¸
MEDIA_MAP = {
    "mk.co.kr": "ë§¤ì¼ê²½ì œ", "hankyung.com": "í•œêµ­ê²½ì œ", "sedaily.com": "ì„œìš¸ê²½ì œ",
    "mt.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´", "edaily.co.kr": "ì´ë°ì¼ë¦¬", "fnnews.com": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
    "bizwatch.co.kr": "ë¹„ì¦ˆì›Œì¹˜", "chosunbiz.com": "ì¡°ì„ ë¹„ì¦ˆ", "asiae.co.kr": "ì•„ì‹œì•„ê²½ì œ",
    "heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ", "dnews.co.kr": "ëŒ€í•œê²½ì œ", "joseilbo.com": "ì¡°ì„¸ì¼ë³´",
    "yna.co.kr": "ì—°í•©ë‰´ìŠ¤", "news1.kr": "ë‰´ìŠ¤1", "newsis.com": "ë‰´ì‹œìŠ¤"
}

def get_financial_indicators():
    try:
        usd_krw = yf.Ticker("USDKRW=X")
        curr = usd_krw.history(period="1d")['Close'].iloc[-1]
        kospi = yf.Ticker("^KS11")
        k_val = kospi.history(period="1d")['Close'].iloc[-1]
        return f"{curr:,.2f}", f"{k_val:,.2f}"
    except:
        return "ë°ì´í„° í™•ì¸ ì¤‘", "ë°ì´í„° í™•ì¸ ì¤‘"

def get_integrated_news():
    """'ê¸ˆìœµ' í‚¤ì›Œë“œë¡œ ìµœì‹  ë‰´ìŠ¤ 100ê°œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    url = f"https://openapi.naver.com/v1/search/news.json?query=ê¸ˆìœµ&display=100&sort=date"
    headers = {"X-Naver-Client-Id": CLIENT_ID, "X-Naver-Client-Secret": CLIENT_SECRET}
    res = requests.get(url, headers=headers)
    return res.json().get('items', []) if res.status_code == 200 else []

def main():
    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # 1. ì‹¤ì‹œê°„ ì§€í‘œ ìˆ˜ì§‘
    rate, kospi = get_financial_indicators()
    
    # 2. í†µí•© ë‰´ìŠ¤ ìˆ˜ì§‘ ë° í•„í„°ë§
    raw_items = get_integrated_news()
    filtered_news = []
    all_titles = []
    unique_titles = set()

    for item in raw_items:
        link = item['link']
        media_name = next((name for domain, name in MEDIA_MAP.items() if domain in link), None)
        
        if media_name:
            title = re.sub(r'<[^>]*>', '', item['title']).replace('&quot;', '"').replace('&apos;', "'")
            if title not in unique_titles:
                filtered_news.append({"date": item['pubDate'][5:16], "media": media_name, "title": title, "link": link})
                all_titles.append(title)
                unique_titles.add(title)

    # 3. ìë™ í‚¤ì›Œë“œ íŠ¸ë Œë“œ ì¶”ì¶œ (ë¹ˆë„ìˆ˜ ë¶„ì„)
    words = []
    stopwords = ['ê¸ˆìœµ', 'ì€í–‰', 'ë‰´ìŠ¤', 'ì˜¤ëŠ˜', 'ê²Œì‹œíŒ', 'ê¸°ì', 'ì¶œì‹œ', 'ê°œìµœ']
    for t in all_titles:
        clean = re.sub(r'[^ê°€-í£\s]', '', t)
        words.extend([w for w in clean.split() if len(w) >= 2 and w not in stopwords])
    trends = [f"`#{tag}`" for tag, _ in Counter(words).most_common(8)]

    # 4. README ì‘ì„±
    news_table = "| ë‚ ì§œ | ì–¸ë¡ ì‚¬ | ë‰´ìŠ¤ í—¤ë“œë¼ì¸ |\n| :--- | :--- | :--- |\n"
    for n in filtered_news[:12]: # ìµœì‹  ë‰´ìŠ¤ 12ê°œ ë…¸ì¶œ
        news_table += f"| {n['date']} | {n['media']} | [{n['title']}]({n['link']}) |\n"

    readme_content = f"""# ğŸ¦ ì‹¤ì‹œê°„ ê¸ˆìœµ/ê²½ì œ ì¢…í•© ë¸Œë¦¬í•‘

> **ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:** `{now}` (KST)  
> **ìë™ ìŠ¤ì¼€ì¤„:** 09:00, 14:00, 17:00 (KST)

---

### ğŸ”¥ ì˜¤ëŠ˜ì˜ AI ì„ ì • í•µì‹¬ í‚¤ì›Œë“œ
{" ".join(trends)}
> *ì£¼ìš” ê²½ì œì§€ 100ê°œ ê¸°ì‚¬ì˜ ì œëª©ì„ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤.*

---

### ğŸ“ˆ ì‹¤ì‹œê°„ ì£¼ìš” ì§€í‘œ
| ì§€í‘œëª… | í˜„ì¬ê°€ |
| :--- | :---: |
| **USD/KRW í™˜ìœ¨** | {rate}ì› |
| **ì½”ìŠ¤í”¼ ì§€ìˆ˜** | {kospi} |

---

### ğŸ“° ì£¼ìš” ê²½ì œì§€ ì‹¤ì‹œê°„ í—¤ë“œë¼ì¸ (TOP 12)
{news_table}

---
*ì œì‘: JiyeonKim017 / ë°ì´í„° ë¶„ì„ ê¸°ë°˜ ìë™ ë‰´ìŠ¤ ë¦¬í¬íŠ¸*
"""
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(readme_content)

if __name__ == "__main__":
    main()
