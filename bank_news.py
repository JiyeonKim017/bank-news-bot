import os
import requests
import yfinance as yf
from datetime import datetime
from collections import Counter
import re
import random

# 1. í™˜ê²½ ë³€ìˆ˜
CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# 2. ì§€ì—°ë‹˜ì˜ 14ê°œ íƒ€ê²Ÿ ë§¤ì²´
MEDIA_MAP = {
    "mk.co.kr": "ë§¤ì¼ê²½ì œ", "mt.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´", "bizwatch.co.kr": "ë¹„ì¦ˆì›Œì¹˜",
    "sedaily.com": "ì„œìš¸ê²½ì œ", "asiae.co.kr": "ì•„ì‹œì•„ê²½ì œ", "edaily.co.kr": "ì´ë°ì¼ë¦¬",
    "chosunbiz.com": "ì¡°ì„ ë¹„ì¦ˆ", "joseilbo.com": "ì¡°ì„¸ì¼ë³´", "fnnews.com": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
    "hankyung.com": "í•œêµ­ê²½ì œ", "heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ",
    "yna.co.kr": "ì—°í•©ë‰´ìŠ¤", "news1.kr": "ë‰´ìŠ¤1", "newsis.com": "ë‰´ì‹œìŠ¤"
}

def get_financial_indicators():
    try:
        usd_krw = yf.Ticker("USDKRW=X")
        rate = usd_krw.history(period="1d")['Close'].iloc[-1]
        kospi = yf.Ticker("^KS11")
        k_val = kospi.history(period="1d")['Close'].iloc[-1]
        return f"{rate:,.2f}", f"{k_val:,.2f}"
    except:
        return "ë°ì´í„° í™•ì¸ ì¤‘", "ë°ì´í„° í™•ì¸ ì¤‘"

def get_balanced_news():
    headers = {"X-Naver-Client-Id": CLIENT_ID, "X-Naver-Client-Secret": CLIENT_SECRET}
    # ê²€ìƒ‰ì–´ë¥¼ 'ê²½ì œ'ë¡œ í†µí•©í•´ì„œ 100ê°œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    url = "https://openapi.naver.com/v1/search/news.json?query=ê²½ì œ&display=100&sort=date"
    res = requests.get(url, headers=headers)
    
    if res.status_code != 200:
        return []
    
    items = res.json().get('items', [])
    filtered_news = []
    unique_titles = set()
    # [í•µì‹¬] ì–¸ë¡ ì‚¬ë³„ ì¹´ìš´íŒ…ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
    media_count = {name: 0 for name in MEDIA_MAP.values()}

    for item in items:
        full_link = item.get('originallink', '') + item.get('link', '')
        media_name = next((name for domain, name in MEDIA_MAP.items() if domain in full_link), None)
        
        if media_name:
            # ì–¸ë¡ ì‚¬ë‹¹ ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ í—ˆìš© (ê³¨ê³ ë£¨ ë³´ì—¬ì£¼ê¸° ìœ„í•´)
            if media_count[media_name] >= 2:
                continue
                
            title = re.sub(r'<[^>]*>', '', item['title']).replace('&quot;', '"').replace('&apos;', "'")
            if title not in unique_titles:
                filtered_news.append({
                    "date": item['pubDate'][5:16],
                    "media": media_name,
                    "title": title,
                    "link": item['link']
                })
                unique_titles.add(title)
                media_count[media_name] += 1
                
        if len(filtered_news) >= 12: # 12ê°œ ì°¨ë©´ ì¢…ë£Œ
            break
            
    return filtered_news

def main():
    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    rate, kospi = get_financial_indicators()
    news_list = get_balanced_news()

    # í‚¤ì›Œë“œ ë¶„ì„ (í•„í„°ë§ëœ ë‰´ìŠ¤ ì œëª© ê¸°ì¤€)
    words = []
    for n in news_list:
        clean = re.sub(r'[^ê°€-í£\s]', '', n['title'])
        words.extend([w for w in clean.split() if len(w) >= 2 and w not in ['ê²½ì œ', 'ë‰´ìŠ¤', 'ì˜¤ëŠ˜']])
    trends = [f"`#{tag}`" for tag, _ in Counter(words).most_common(6)]

    # ë‰´ìŠ¤ í…Œì´ë¸” ì‘ì„±
    news_table = "| ë‚ ì§œ | ì–¸ë¡ ì‚¬ | ë‰´ìŠ¤ í—¤ë“œë¼ì¸ |\n| :--- | :--- | :--- |\n"
    for n in news_list:
        news_table += f"| {n['date']} | {n['media']} | [{n['title']}]({n['link']}) |\n"

    readme_content = f"""# ğŸ¦ ì‹¤ì‹œê°„ ê²½ì œ ì¢…í•© ë¸Œë¦¬í•‘ (14ê°œ ë§¤ì²´ ê· í˜•)

> **ì—…ë°ì´íŠ¸:** `{now}` (KST)

---

### ğŸ”¥ ì˜¤ëŠ˜ì˜ í‚¤ì›Œë“œ
{" ".join(trends)}

---

### ğŸ“ˆ ì£¼ìš” ì§€í‘œ
| ì§€í‘œëª… | í˜„ì¬ê°€ |
| :--- | :---: |
| **USD/KRW í™˜ìœ¨** | {rate}ì› |
| **ì½”ìŠ¤í”¼ ì§€ìˆ˜** | {kospi} |

---

### ğŸ“° ì‹¤ì‹œê°„ ì£¼ìš” ë‰´ìŠ¤ (ë§¤ì²´ë³„ ê· í˜• ì„ ë³„)
{news_table}

---
*ì œì‘: JiyeonKim017 / 11ê°œ ê²½ì œì§€ + 3ê°œ í†µì‹ ì‚¬ ê¸°ë°˜*
"""
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(readme_content)

if __name__ == "__main__":
    main()
